from fastapi import FastAPI
import redis
import os 
from langchain_community.llms import CTransformers # CTransformers ì„í¬íŠ¸
from langchain_core.prompts import PromptTemplate 
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser 

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì„í¬íŠ¸ (ë³€ê²½ ì—†ìŒ)
from db_utils import init_db, save_info, query_info, retrieve_context
from memory import set_session_data, get_session_data, append_chat_history, get_chat_history, clear_session

# schemas.pyì—ì„œ ì •ì˜ëœ Pydantic ëª¨ë¸ì„ ì„í¬íŠ¸ (ë³€ê²½ ì—†ìŒ)
from schemas import ChatPayload, AgentActionSchema 

# -----------------------------
# ì´ˆê¸°í™”
# -----------------------------
init_db() # SQLite DB ì´ˆê¸°í™”

# ğŸ’¡ Redis ì—°ê²° ìƒíƒœ í™•ì¸ 
try:
    redis_client_check = redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)
    redis_client_check.ping()
    print("âœ… Redis ì„œë²„ ì—°ê²° ì„±ê³µ. ì„¸ì…˜ ê´€ë¦¬ê°€ í™œì„±í™”ë©ë‹ˆë‹¤.")
except redis.exceptions.ConnectionError:
    print("âŒ Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„¸ì…˜ ê´€ë¦¬ëŠ” ë”ë¯¸ ê°ì²´ë¡œ ë™ì‘í•©ë‹ˆë‹¤. Redisê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
    
# -----------------------------
# LLM ì´ˆê¸°í™” ë° ìŠ¤í‚¤ë§ˆ ì •ì˜ (â˜…â˜…â˜… CTransformersë¡œ ë³€ê²½ â˜…â˜…â˜…)
# -----------------------------
# CTransformers ì„¤ì •
MODEL_PATH = "local_model.gguf" # ë‹¤ìš´ë¡œë“œí•œ GGUF íŒŒì¼ ê²½ë¡œ (í˜„ì¬ í´ë”ì— ìˆì–´ì•¼ í•¨)

if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. '{MODEL_PATH}' íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì´ í´ë”ì— ì €ì¥í•´ì£¼ì„¸ìš”.")

# ëª¨ë¸ ì„¤ì • (CPU ê¸°ë°˜ ì‹¤í–‰ì„ ìœ„í•œ ê¸°ë³¸ ì„¤ì •)
config_agent = {'max_new_tokens': 100, 'temperature': 0.1, 'context_length': 2048}
config_general = {'max_new_tokens': 512, 'temperature': 0.7, 'context_length': 4096}


# CTransformers LLM ì´ˆê¸°í™” (Action íŒë‹¨ìš© - ì˜¨ë„ ë‚®ê²Œ)
llm_agent = CTransformers(
    model=MODEL_PATH, 
    model_type="mistral", # ì‚¬ìš©í•œ ëª¨ë¸ì— ë”°ë¼ ë³€ê²½ (Nous-Hermes-2-Mistral-7B-DPOëŠ” mistral ê¸°ë°˜)
    config=config_agent
)

# CTransformers LLM ì´ˆê¸°í™” (ì¼ë°˜ ëŒ€í™”ìš© - ì˜¨ë„ ë†’ê²Œ)
llm_general = CTransformers(
    model=MODEL_PATH, 
    model_type="mistral", 
    config=config_general
) 

# Agent Action íŒë‹¨ìš© Parser
parser = JsonOutputParser(pydantic_object=AgentActionSchema)
# ì¼ë°˜ ëŒ€í™”ìš© Parser (í…ìŠ¤íŠ¸ ì¶œë ¥)
general_parser = StrOutputParser()

# -----------------------------
# FastAPI ì‹œì‘
# -----------------------------
app = FastAPI()

# -----------------------------
# LLM ì—ì´ì „íŠ¸: í–‰ë™ íŒë‹¨ í•¨ìˆ˜
# -----------------------------
def parse_user_action(message: str, history: list[str]) -> dict:
    """LLMì—ê²Œ Pydantic ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ì  ì¶œë ¥ ê°•ì œ"""
    
    # CTransformers LLMì€ Chat ëª¨ë¸ì´ ì•„ë‹ˆë¯€ë¡œ, System Promptë¥¼ Prompt Templateì— í¬í•¨í•©ë‹ˆë‹¤.
    # LLMì´ JSONì„ ì˜ ìƒì„±í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ì— ëª…ì‹œì ìœ¼ë¡œ ì§€ì‹œ
    sys_prompt_content = f"""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë³´ê³  í–‰ë™(action)ì„ ê²°ì •í•˜ëŠ” ì „ë¬¸ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì‚¬ìš©ìê°€ ì •ë³´ë¥¼ ì €ì¥í•˜ë ¤ëŠ”ì§€(action: 'save'), ì¡°íšŒí•˜ë ¤ëŠ”ì§€(action: 'query'), ì•„ë‹ˆë©´ ì¼ë°˜ ëŒ€í™”(action: 'none')ë¥¼ í•˜ë ¤ëŠ”ì§€ íŒë‹¨í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.
categoryëŠ” 'interest' ë˜ëŠ” 'study'ë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
ë°˜ë“œì‹œ ì•„ë˜ ìŠ¤í‚¤ë§ˆì— ë”°ë¼ **ì •í™•íˆ JSON í˜•ì‹**ìœ¼ë¡œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

ìŠ¤í‚¤ë§ˆ:
{parser.get_format_instructions()}
"""

    prompt_template = PromptTemplate(
        template="{system_prompt}\n\nëŒ€í™” ê¸°ë¡:\n{conversation}\nì‚¬ìš©ì ì…ë ¥:\n{message}",
        input_variables=["system_prompt", "conversation", "message"]
    )
    
    conversation = "\n".join(history)
    
    # LLM Chain: Prompt -> LLM (CTransformers) -> Parser (JSON)
    chain = prompt_template | llm_agent | parser

    try:
        # LLM ì‘ë‹µì„ Pydantic ê°ì²´ë¡œ ë³€í™˜í•˜ê³  dictë¡œ ë°˜í™˜
        return chain.invoke({
            "system_prompt": sys_prompt_content,
            "conversation": conversation,
            "message": message
        })
    except Exception as e:
        print(f"Agent Parsing Error: {e}")
        # íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ none action ë°˜í™˜
        return {"action": "none"}

@app.post("/chat")
def chat_endpoint(payload: ChatPayload):
    session_id = payload.session_id
    msg = payload.message.strip()

    session = get_session_data(session_id)
    append_chat_history(session_id, "user", msg)
    history = get_chat_history(session_id)
    
    # -----------------------------
    # â‘  ì¸ì¦ ìš”ì²­/í™•ì¸ ë¡œì§
    # -----------------------------
    USER_VERIFY_CODE = "abcd"
    
    if session.get("saving_mode") and not session.get("user_verified"):
        if msg == USER_VERIFY_CODE:
            session["user_verified"] = True
            set_session_data(session_id, session)
            return {"response": "ë³¸ì¸ ì¸ì¦ ì™„ë£Œ! ì´ì œ ì •ë³´ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ 'ì €ì¥ ë'ì´ë¼ê³  ë§ì”€í•´ì£¼ì„¸ìš”."}
        elif msg in ["ì €ì¥ ë", "ì·¨ì†Œ"]:
            session["saving_mode"] = False
            session["user_verified"] = False
            set_session_data(session_id, session)
            return {"response": "ì €ì¥ ëª¨ë“œê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."}
        else:
            return {"response": "ì •ë³´ ì €ì¥ì„ ìœ„í•´ì„œëŠ” ë¨¼ì € ë³¸ì¸ ì¸ì¦ ì½”ë“œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}

    # -----------------------------
    # â‘¡ LLMì—ê²Œ í–‰ë™(action) íŒë‹¨ ìš”ì²­
    # -----------------------------
    agent_output = parse_user_action(msg, history)
    action = agent_output.get("action")
    category = agent_output.get("category")
    value = agent_output.get("value")

    # -----------------------------
    # â‘¢ Action ì‹¤í–‰
    # -----------------------------
    if action == "save":
        if not session.get("user_verified"):
            # ì¸ì¦ ì•ˆ ë˜ì–´ ìˆìœ¼ë©´ ì¸ì¦ ëª¨ë“œë¡œ ì „í™˜ ë° ìš”ì²­
            session["saving_mode"] = True
            set_session_data(session_id, session)
            return {"response": "ì •ë³´ ì €ì¥ì„ ìœ„í•´ ë¨¼ì € ë³¸ì¸ ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤. 'abcd'ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”."}
            
        if category and value and category in ["interest", "study"]:
            save_info(category, value) # db_utils.save_info ì‚¬ìš©
            append_chat_history(session_id, "assistant", f"ì‚¬ìš©ì ì •ë³´ ì €ì¥ë¨: {category}: {value}")
            return {"response": f"ì €ì¥í–ˆìŠµë‹ˆë‹¤! -> ì¹´í…Œê³ ë¦¬: {category}, ë‚´ìš©: {value}"}
        else:
            # save ì˜ë„ëŠ” ìˆìœ¼ë‚˜ ì¹´í…Œê³ ë¦¬/ê°’ì´ ë¶ˆëª…í™•í•  ê²½ìš° ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
            action = "none" 
            
    if action == "query":
        if category and category in ["interest", "study"]:
            rows = query_info(category) # db_utils.query_info ì‚¬ìš©
            append_chat_history(session_id, "assistant", f"ì‚¬ìš©ì ì •ë³´ ì¡°íšŒë¨: {category}")
            
            if rows:
                return {"response": f"ë‹¹ì‹ ì˜ '{category}' ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: {', '.join(rows)}"}
            else:
                return {"response": f"'{category}'ì— ëŒ€í•œ ì €ì¥ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."}
        else:
            # query ì˜ë„ëŠ” ìˆìœ¼ë‚˜ ì¹´í…Œê³ ë¦¬ê°€ ë¶ˆëª…í™•í•  ê²½ìš° ì¼ë°˜ ëŒ€í™”ë¡œ ì²˜ë¦¬
            action = "none" 

    if action == "none" and msg in ["ì €ì¥ ë", "ì €ì¥ ì™„ë£Œ"]:
        session["saving_mode"] = False
        session["user_verified"] = False
        set_session_data(session_id, session)
        return {"response": "ì •ë³´ ì €ì¥ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."}
    
    # -----------------------------
    # â‘£ ì¼ë°˜ ëŒ€í™” (RAG í†µí•©) ì²˜ë¦¬
    # -----------------------------
    if action in ["none", "other"]:
        # ğŸ’¡ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¡°íšŒ
        context = retrieve_context(msg) # db_utils.retrieve_context ì‚¬ìš©
        
        # ì¼ë°˜ ëŒ€í™”ìš© Prompt Template ì •ì˜
        general_prompt_template = PromptTemplate(
            template="""
ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ìŒ ëŒ€í™” ê¸°ë¡ê³¼ ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
ì•„ë˜ì˜ [DB Context]ëŠ” ì‚¬ìš©ìì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ê´€ë ¨ ì§ˆë¬¸ì— ëŒ€í•´ ì´ ì •ë³´ë¥¼ í™œìš©í•˜ì„¸ìš”.

[DB Context]: 
{context}

ëŒ€í™” ê¸°ë¡: {history}
ì‚¬ìš©ì ì…ë ¥: {msg}
""",
            input_variables=["context", "history", "msg"]
        )
        
        # LLM Chain: Prompt -> LLM (CTransformers) -> Parser (Str)
        chain = general_prompt_template | llm_general | general_parser

        try:
            res = chain.invoke({
                "context": context if context else 'ì €ì¥ëœ ì •ë³´ ì—†ìŒ',
                "history": "\n".join(history),
                "msg": msg
            })
            response_text = res.strip()
            append_chat_history(session_id, "assistant", response_text)
            return {"response": response_text}
        except Exception as e:
            print(f"General LLM Error: {e}")
            return {"response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§€ê¸ˆì€ ëŒ€í™” ì²˜ë¦¬ê°€ ì–´ë µìŠµë‹ˆë‹¤."}

    return {"response": "ë¬´ìŠ¨ ë§ì¸ì§€ ì˜ ëª¨ë¥´ê² ì–´ìš”."}